{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "268aba7ccc796b6abe1bb678b85824df",
     "grade": false,
     "grade_id": "cell-5cb269cc88b84f7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h1><center>Gradient Based Optimization</center></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for CS-EJ3311 - Deep Learning with Python</font></center>\n",
    "<center><font size=\"3\">23.10.-10.12.2023</font></center>\n",
    "<center><font size=\"3\">Aalto University & FiTech.io</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a79910e866142e37b42f0d0e03f93392",
     "grade": false,
     "grade_id": "cell-ca2bcb5604e982c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-danger\">\n",
    "   <h3 align=\"center\"> <b>NOTE:</b></h3><br>These notebooks were prepared as classic jupyter notebooks and may look different in jupyter lab. To switch to classic notebook from Jupyter Lab go to Help > Launch Classic Notebook.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c341a72b79cdc4329fabf30c2b655ba3",
     "grade": false,
     "grade_id": "cell-f67d71d3c0644230",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This notebook demonstrates a simple but powerful method to find an optimal choice for the parameters (weights and biases) of an artificial neural network (ANN). The idea is to tune (adjust) the parameters according to the gradient of the average loss incurred by the ANN on a training set. This average loss is also known as the **training loss** (or training error) and defines an **objective or cost function** $f(\\mathbf{w})$ that we want to minimize.  \n",
    "\n",
    "Here we discuss a simple iterative algorithm which is called **gradient descent** (GD). GD minimizes the training loss by incrementally improving the current guess for the optimal parameters by moving a bit into the direction of the negative gradient. We will also discuss a simple variant of GD known as **stochastic gradient descent** (SGD). SGD is one of the most widely used optimization methods within deep learning.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- understand how gradients of the loss function can be used to learn the parameters of an ANN\n",
    "\n",
    "- understand the basic idea behind stochastic gradient descent (SGD)\n",
    "\n",
    "- understand SGD components \"batch\", \"batch size\", \"learning rate\" and \"epoch\"\n",
    "\n",
    "- be aware of some advanced variants of SGD such as ADAM or RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec01ff6f6cc1279ef31b5d40e018c6c0",
     "grade": false,
     "grade_id": "cell-7f83fda88459b81f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Additional Reading\n",
    "\n",
    "-  Loss function chapter 1.1.5 & Gradient-based optimization chapter 2.4 of the book \"Deep Learning with Python\" by F. Chollet. \n",
    "-  Gradient descent Chapter 4 & 11 of \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron. \n",
    "\n",
    "Advanced reading:\n",
    "\n",
    "- Chapter 2.3 & 5 of [\"Machine Learning: The Basics\"](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf) by Alex Jung \n",
    "- Chapter 8 [\"Optimization for Training DeepModels\"](https://www.deeplearningbook.org/contents/optimization.html) of Deep Learning Book by Ian Goodfellow \n",
    "\n",
    "## Video materials\n",
    "\n",
    "Beginner-friendly videos:\n",
    "- 3Blue1Brown, [\"Gradient descent, how neural networks learn\"](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- StatQuest, [\"Gradient Descent, Step-by-Step\"](https://www.youtube.com/watch?v=sDv4f4s2SB8)\n",
    "\n",
    "Videos from Andrew Ng's courses  with a brief intro to:\n",
    "- [\"Gradient Descent (C1W2L04)\"](https://www.youtube.com/watch?v=uJryes5Vk1o)\n",
    "- [\"Adam Optimization Algorithm (C2W2L08)\"](https://www.youtube.com/watch?v=JXQT_vxqwIs)\n",
    "- [\"RMSProp (C2W2L07)\"](https://www.youtube.com/watch?v=_e-LFe_igno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92d6cf44621b6a5ae7931558439c9f31",
     "grade": false,
     "grade_id": "cell-be6ce98897a3b562",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: 'Sanchez';\n",
       "  src: url('https://fonts.googleapis.com/css?family=Sanchez:400italic,400');\n",
       "}\n",
       "\n",
       "@import url('https://fonts.googleapis.com/css2?family=Sanchez&display=swap');\n",
       "\n",
       "* {\n",
       "  margin: 0;\n",
       "  padding: 0;\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "\n",
       "*,\n",
       "*:before,\n",
       "*:after {\n",
       "\tbox-sizing: inherit;\n",
       "}\n",
       "\n",
       "body {\n",
       "font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica,\n",
       "    Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\";\n",
       "}\n",
       "\n",
       ".title-container {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       ".title {\n",
       "  font-weight: 600;\n",
       "}\n",
       "\n",
       ".subtitle {\n",
       "  margin: 10px 0px;\n",
       "  color: #888888;\n",
       "  font-size: 25px;\n",
       "  transition: all 0.5s;\n",
       "}\n",
       "\n",
       ".main-container {\n",
       "  padding: 15px;\n",
       "}\n",
       "\n",
       ".card-container {\n",
       "  display: flex;\n",
       "  flex-wrap: wrap;\n",
       "  justify-content: space-between;\n",
       "}\n",
       "\n",
       ".card {\n",
       "  margin: 20px;\n",
       "  padding: 20px;\n",
       "  width: 100%;\n",
       "  min-height: 200px;\n",
       "  display: grid;\n",
       "  grid-template-columns: 1fr 1fr 1fr;\n",
       "  gap: 10px;\n",
       "  border-radius: 10px;\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.25);\n",
       "  transition: all 0.5s;\n",
       "}\n",
       "\n",
       ".card.small {\n",
       "  width: 50%;\n",
       "  min-height: 100px;\n",
       "  grid-template-columns: 1fr 1fr;\n",
       "}\n",
       "\n",
       ".card:hover {\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.4);\n",
       "  transform: scale(1.01);\n",
       "}\n",
       "\n",
       ".card__title {\n",
       "  grid-columnn-start: 1;\n",
       "  grid-columnn-end: -1;\n",
       "  font-weight: 400;\n",
       "  color: #ffffff;\n",
       "}\n",
       "\n",
       ".test-input {\n",
       "  grid-column-start: 1;\n",
       "  grid-column-end: 2;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".test-output {\n",
       "  grid-column-start: 2;\n",
       "  grid-column-end: 3;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".test-expected-output {\n",
       "  grid-column-start: 3;\n",
       "  grid-column-end: 4;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".card-failure {\n",
       "  background: radial-gradient(#fbc1cc, #fa99b2);\n",
       "}\n",
       "\n",
       ".card-failure .card__title::before {\n",
       "    display: inline-block;\n",
       "    margin-right: 5px;\n",
       "    font-style: normal;\n",
       "    font-variant: normal;\n",
       "    text-rendering: auto;\n",
       "    -webkit-font-smoothing: antialiased;\n",
       "    font-family: \"Font Awesome 5 Free\";\n",
       "    font-weight: 900;\n",
       "    content: \"\\f057\";\n",
       "}\n",
       "\n",
       ".card-success {\n",
       "  background: radial-gradient(#60efbc, #58d5c9);\n",
       "}\n",
       "\n",
       ".card-success .card__title::before {\n",
       "    display: inline-block;\n",
       "    margin-right: 5px;\n",
       "    font-style: normal;\n",
       "    font-variant: normal;\n",
       "    text-rendering: auto;\n",
       "    -webkit-font-smoothing: antialiased;\n",
       "    font-family: \"Font Awesome 5 Free\";\n",
       "    font-weight: 900;\n",
       "    content: \"\\f058\";\n",
       "}\n",
       "\n",
       ".card-info {\n",
       "  background: radial-gradient(#1fe4f5, #3fbafe);\n",
       "}\n",
       "\n",
       "@media (max-width: 1600px) {\n",
       "  .card-container {\n",
       "    justify-content: center;\n",
       "  }\n",
       "}\n",
       "\n",
       ".code-block {\n",
       "  padding: 5px;\n",
       "  background-color: #f3f7f7;\n",
       "  color: black;\n",
       "  border-radius: 10px;\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.25);\n",
       "}\n",
       "\n",
       "details {\n",
       "\tfont-size: 1rem;\n",
       "\tbox-shadow: 0 10px 15px -5px rgba(0, 0, 0, 0.1),\n",
       "\t\t0 10px 10px -5px rgba(0, 0, 0, 0.04);\n",
       "\twidth: 100%;\n",
       "\tbackground: #ffffff;\n",
       "\tborder-radius: 10px;\n",
       "\tposition: relative;\n",
       "}\n",
       "\n",
       "details:hover {\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".summary-title {\n",
       "    user-select: none;\n",
       "    margin-left: 5px;\n",
       "}\n",
       "\n",
       ".summary-content {\n",
       "    border: 2px solid #0C7B89;\n",
       "    cursor: default;\n",
       "    padding: 1em;\n",
       "    font-weight: 300;\n",
       "    font-size: 15px;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       "summary {\n",
       "   color: white;\n",
       "   font-size: large;\n",
       "   font-weight: bold;\n",
       "   padding: 1em;\n",
       "   background-color: #0C7B89;\n",
       "   border-radius: 8px;\n",
       "   list-style: none;\n",
       "}\n",
       "\n",
       "details[open] summary {\n",
       "    border-radius: 8px 8px 0 0;\n",
       "}\n",
       "\n",
       "details[open] summary::before {\n",
       "  transform: rotate(90deg);\n",
       "  font-family: \"Font Awesome 5 Free\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       "details summary::before {\n",
       "  position: absolute;\n",
       "  will-change: transform;\n",
       "  transition: transform 300ms ease;\n",
       "  font-family: \"Font Awesome 5 Free\";\n",
       "  color: #fff;\n",
       "  font-size: 1.1rem;\n",
       "  content: \"\\f105\";\n",
       "  left: 0;\n",
       "  display: inline-block;\n",
       "  width: 1.6rem;\n",
       "  text-align: center;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       "summary:focus {\n",
       "  outline: none;\n",
       "}\n",
       "\n",
       "summary::-webkit-details-marker {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       ".ludwig {\n",
       "  position: relative;\n",
       "  padding-left: 1em;\n",
       "  border-left: 0.2em solid lighten( hsl(200, 40, 10), 40%);\n",
       "  font-family: 'Roboto', serif;\n",
       "  font-size: 1.3em;\n",
       "  line-height: 1.5em;\n",
       "  font-weight: 100;\n",
       "}\n",
       "\n",
       ".ludwig::before, .ludwig::after {\n",
       "  content: '\\201C';\n",
       "  font-family: 'Sanchez';\n",
       "  color: lighten( hsl(200, 40, 10), 40%);\n",
       "}\n",
       "\n",
       ".ludwig::after {\n",
       "  content: '\\201D';\n",
       "}\n",
       "\n",
       ".blockquote-container {\n",
       "  margin: 2em auto;\n",
       "}\n",
       "\n",
       "blockquote {\n",
       "  margin-bottom: 3em;\n",
       "}\n",
       "\n",
       ".wrap-up {\n",
       "    border: 2px solid #B41086;\n",
       "    cursor: default;\n",
       "    padding: 20px;\n",
       "    line-height: 1.5;\n",
       "    border-radius: 20px\n",
       "}\n",
       "\n",
       ".wrap-up-title {\n",
       "    font-size:20px;\n",
       "    color: #B41086;\n",
       "    font-weight: bold\n",
       "}\n",
       "\n",
       ".wrap-up-content {\n",
       "    font-size:14px;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".info {\n",
       "    background-color: white;\n",
       "    border: 2px solid #0C7B89;\n",
       "    cursor: default;\n",
       "    padding: 10px;\n",
       "    line-height: 1.5;\n",
       "    border-radius: 20px\n",
       "}\n",
       "\n",
       ".info-title {\n",
       "    font-size: 20px;\n",
       "    color: #0C7B89;\n",
       "    font-weight: bold\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_styles\n",
    "\n",
    "# This MUST be the last line of this cell\n",
    "load_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02053fb49b07b16be2bef35e4b6b10ac",
     "grade": false,
     "grade_id": "cell-b0e3481a9dd3e926",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Deep learning aims at finding a good choice for the weights (and biases) of an **artificial neural network (ANN)**. We need to define a loss function to measure how \"good\" a particular choice for these parameters is. For a given pair of predicted label value $\\hat{y}$ and true label value $y$, the loss function $L(y,\\hat{y})$ provides a measure for the error, or \"loss\", incurred in predicting the true label $y$ by $\\hat{y}$. We emphasize that the precise definition of the loss function is a design choice. In principle, the deep learning engineer is free to choose an arbitrary loss function used to guide the training of an ANN.  \n",
    "\n",
    "Some particular choices for the loss function have been proven to be useful in many applications. If the label values are numeric (like a temperature or a weight), then the **squared error** loss $ L(y,\\hat{y})=(y-\\hat{y})^2$ is a good choice for the loss function. \\\n",
    "If the label values are categories (like \"cat\" and \"dog\"), we might use  the **logistic loss** $L(y,\\hat{y}) = -y\\ln\\big(\\hat{p}(y=1)\\big)-(1-y)\\ln\\big(\\hat{p}(y=0)\\big)$ where $\\hat{p}$ is an estimate for the probability that label value is $y=1$ or $y=0$.\n",
    "\n",
    "To measure the quality of a particular choice for the parameters of an ANN, we typically use a set of labeled data points \n",
    "\n",
    "$$\\big(\\mathbf{x}^{(1)},y^{(1)}\\big),\\ldots,\\big(\\mathbf{x}^{(m)},y^{(m)}\\big).$$\n",
    "\n",
    "We first compute the resulting predictions $\\hat{y}^{(i)}$ obtained when feeding the feature vectors $\\mathbf{x}^{(i)}$ into the ANN. Based on the predictions, we calculate the average loss (or training loss)\n",
    "\n",
    "$$ (1/m) \\big( L(y^{(1)},\\hat{y}^{(1)})+L(y^{(2)},\\hat{y}^{(2)})+\\ldots+L(y^{(m)},\\hat{y}^{(m)}) \\big).$$\n",
    "\n",
    "Note that the training loss depends on the parameters $\\mathbf{w}$ of the ANN via the predictions $\\hat{y}^{(i)}$. Indeed, the predictions $\\hat{y}^{(i)}=h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$ are obtained by applying the ANN with parameters $\\mathbf{w}$ to the input feature vector $\\mathbf{x}^{(i)}$. By evaluating the training loss for different choices for the weights, we obtain a **cost or objective function** $f(\\mathbf{w})$. The objective function $f(\\mathbf{w})$ is the average loss incurred by an ANN with parameters $\\mathbf{w}$. It seems natural to learn parameters of an ANN that minimize the training loss, i.e., that solve\n",
    "\n",
    "$$ \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eab26419cd837be8116d953a326bd15b",
     "grade": false,
     "grade_id": "cell-fd3af423ccbcda83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"info\">\n",
    "    <div  class=\"info-title\"><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Info</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "         Here we use following format:\n",
    "        <br>\n",
    "        <ul>\n",
    "          <li>non-bold font and lower case for scalar variables ( true label $y$, predicted label $\\hat{y}$ )</li>\n",
    "          <li>bold font and lower case for vectors ( features of a datapoint stacked into a vector $\\mathbf{x}$, ANN parameters stacked into a vector $\\mathbf{w}$ )</li>\n",
    "          <li>bold font and upper case for matrices ( feature matrix $\\mathbf{X}$ )</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bdc290d9797235132e69618b97918ee",
     "grade": false,
     "grade_id": "cell-74b899199a3ac3b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Mean Squared Error \n",
    "\n",
    "Maybe the most widely used loss function for applications involving numeric label values $y \\in \\mathbb{R}$ is the squared error loss \n",
    "\n",
    "$$L(y,\\hat{y}) = (\\underbrace{y- \\hat{y}}_{\\mbox{prediction error}})^{2}.$$\n",
    "\n",
    "We assess the quality of a predictor $\\hat{y} = h^{(\\mathbf{w})}(\\mathbf{x})$ by the average loss incurred over a set of labeled data points (the **training set**). For the squared error, average loss is referred to as the **mean squared error (MSE)** \n",
    "\n",
    "$$ f(\\mathbf{w}) = (1/m) \\big( \\big( y^{(1)}-\\hat{y}^{(1)}\\big)^{2}+\\big( y^{(2)}-\\hat{y}^{(2)}\\big)^{2}+\\ldots+\\big( y^{(m)}-\\hat{y}^{(m)}\\big)^{2} \\big). $$\n",
    "\n",
    "Note that the MSE on the right hands side depends on the weight vector $\\mathbf{w}$ via the predictions $\\hat{y}^{(i)}$ obtained by applying the predictor map $h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9c97b7a9653de0fcde76c238a9aa3ea",
     "grade": false,
     "grade_id": "cell-7a19ba4e42949ab2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The shape of the loss $f(\\mathbf{w})$, viewed as a function of the weights $\\mathbf{w}$, depends on two components. First, it depends on how the predictor map depends on the weights (the structure of the ANN in deep learning context). Second, it depends on the choice of the loss function $L(y,\\hat{y})$ used to measure the loss incurred by predicting the true label value $y$ with the prediction $\\hat{y}$. \n",
    "\n",
    "The combination of linear predictor function and squared error loss $L(y,\\hat{y})=(y-\\hat{y})^{2}$ is a very popular as they result in a [convex](https://en.wikipedia.org/wiki/Convex_function) and [differentiable](https://en.wikipedia.org/wiki/Differentiable_function) loss function $f(\\mathbf{w})$. A convex function has the attractive property that any local minimum is always also a [global minimum](https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg). If a convex function is also differentiable, it can be minimized by a simple but powerful algorithm which is known as **gradient descent**. \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/MSELinPred.jpeg\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1926db6782080c70661c29fa9ac2d330",
     "grade": false,
     "grade_id": "cell-a24f9b508deb7119",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Deep learning methods use predictor maps represented by ANN with tunable weights. In this case, the predictor depends non-linearly on the weights. As a result, we obtain (highly) non-convex loss landscapes. Below, examples of loss function landscapes of more complicated models (neural networks) illustrate that finding a minimum of these loss functions is not a trivial task.\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/NNloss.png\" width=500/>\n",
    "\n",
    "\n",
    "<center><a href=\"https://www.cs.umd.edu/~tomg/projects/landscapes/\">image source</a></center>\n",
    "<center><a href=\"https://arxiv.org/abs/1712.09913/\">original paper</a></center>\n",
    "\n",
    "Here you can find more examples of visualizations for loss functions obtained from representing a predictor map using ANN:\n",
    "\n",
    "[3D visualization of NN loss functions](http://www.telesens.co/loss-landscape-viz/viewer.html) \\\n",
    "[3D animation of NN loss functions](https://www.youtube.com/watch?time_continue=32&v=aq3oA6jSGro&feature=emb_logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8b376a79df6552ae6798970feff5b4f",
     "grade": false,
     "grade_id": "cell-bc3add6f815d8e8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Gradient Descent \n",
    "\n",
    "We will now introduce a simple algorithm that allows to find (a good approximation to) the optimal parameter vector $\\mathbf{w}_{\\rm opt}$ for a predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$. The optimum weight vector should result in the smallest possible loss  \n",
    "\n",
    "\\begin{align} \n",
    "f(\\mathbf{w}_{\\rm opt}) = \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w}) \\mbox{ with } f(\\mathbf{w})& = (1/m) \\sum_{i=1}^{m} \\big(y^{(i)} - \\hat{y}^{(i)} \\big)^{2} \\nonumber \\\\ \n",
    "& =(1/m) \\sum_{i=1}^{m} \\big(y^{(i)} - h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big) \\big)^{2}. \\end{align}\n",
    "\n",
    "**Gradient descent (GD)** constructs a sequence of parameter vectors $\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\ldots$ such that the loss values $f\\big(\\mathbf{w}^{(0)}\\big),f\\big(\\mathbf{w}^{(1)}\\big),\\ldots$ tends toward the minimum loss. GD is an iterative algorithm that gradually improves the current guess (approximation) $\\mathbf{w}^{(k)}$ for the optimum weight vector.  \n",
    "\n",
    "There are many different strategies for choosing the first (or initial) guess $\\mathbf{w}^{(0)}$. One simple approach is to choose the initial weights randomly. **Given the current weight vector $\\mathbf{w}^{(k)}$, how does GD know in which \"direction\" to go to find a better parameter vector $\\mathbf{w}^{(k+1)}$?** Mathematics, or [calcululs](https://en.wikipedia.org/wiki/Differential_calculus) to be specific, tells us that this direction is precisely the opposite of the gradient $\\nabla f(\\mathbf{w})$. More precisely, for a small step size, the steepest descent is towards the opposite direction of the [gradient](https://en.wikipedia.org/wiki/Derivative). We can think of [GD](https://en.wikipedia.org/wiki/Gradient_descent) as imitating a hiker who takes a sequence of (small) steps downhill.\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/GradientHiker.jpeg\" width=500/>\n",
    "\n",
    "Given the downhill direction $- \\nabla f\\big(\\mathbf{w}^{(k)}\\big)$ at the current estimate $\\mathbf{w}^{(k)}$, we take a step \n",
    "\n",
    "$$\\mbox{(Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha}_{\\mbox{step size}} \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "\n",
    "Here, we used a tuning parameter $\\alpha>0$ which adjusts the step size for the steep downhill. We will refer to this parameter as **learning rate**. This name is due to the fact that choosing a larger value for $\\alpha$ tends to speed up the progress of GD to reach the optimum weight vector. Thus, increasing the value of $\\alpha$ tends to speed up the learning of a good weight vector for a predictor map $h^{(\\mathbf{w})}$. \n",
    "\n",
    "The GD algorithm requires the specification of a suitable learning rate  $\\alpha$ and initial guess $\\mathbf{w}^{(0)}$ and then repeating the gradient step for a sufficient number of iterations. One possible stopping criterion is to use a fixed number of iterations which might be dictated by constraints on processing duration we grant for GD (computing time costs money, [see here](https://aws.amazon.com/emr/pricing/)). \n",
    "\n",
    "Another option is to monitor the loss function and stop if consecutive iterates do not result in any significant decrease. Similarly, we could monitor the validation loss which is obtained by applying the predictor map using the current GD iterate $\\mathbf{w}^{(k)}$ to validation data which is different from the training data used to define the training loss. \n",
    "\n",
    "A key challenge in the use of GD is to find a good choice for the learning rate $\\alpha$. If the learning rate is too small (left plot below), the GD steps make too little progress and thus require an excessive number of iterations to get close to the optimum weight vector. Conversely, if the learning rate is too high (right plot below), it is possible that GD iterates $\\mathbf{w}^{(k)}$ will \"overshoot\" the minimum and climb up the loss function on the other side of the minimum (GD diverges). \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/lrate.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23844f355103c4ec0b6b0a526c3ed672",
     "grade": false,
     "grade_id": "cell-924816fce21eb6a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The figure below shows how GD adapts the parameter vector $\\mathbf{w}$ of a linear predictor $h^{(\\mathbf{w})}(\\mathbf{x})= \\mathbf{w}^{T} \\mathbf{x}$ to better fit the labeled data points (left) resulting in a smaller MSE (right). Note that after around $200$ iterations, gradient descent found weight vectors resulting in an almost minimum MSE. The additional iterations (beyond $200$) are (in some sense) a waste of computation as they do not decrease the MSE significantly. \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/plainGD.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb9b089c352872028a1d77e68121d1d",
     "grade": false,
     "grade_id": "cell-140d9675efb76a99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To develop some intuition for the functioning of GD, let us work out the gradient update for the special case of linear predictor maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. Here, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The gradient update of GD then becomes, in turn, \n",
    "\n",
    "\\begin{align} \n",
    "\\mathbf{w}^{(k+1)} & = \\mathbf{w}^{(k)} + {\\alpha}\\,(2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big)\\nonumber \\\\ \n",
    " & = \\mathbf{w}^{(k)} + {\\alpha}\\,(2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big( y^{(i)} - \\hat{y}^{(i)} \\big) \n",
    ". \\end{align}\n",
    "\n",
    "Note that the gradient update involves the computation of the predictions $\\hat{y}^{(i)} = \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)}$. \n",
    "\n",
    "After the forward pass, the weight vector $\\mathbf{w}^{(k)}$ is updated by a weighted combination of the feature vectors $\\mathbf{x}^{(i)}$. The weight for the $i$th feature vector $\\mathbf{x}^{(i)}$ is given by the prediction error $ \\big( y^{(i)}$ - $\\hat{y}^{(i)} \\big)$ incurred by the current parameter vector for that data point. Thus, the gradient update puts more emphasis (larger weight) on those data points $\\big(\\mathbf{x}^{(i)},y^{(i)}\\big)$ which are not well predicted using the current weight vector $\\mathbf{w}^{(k)}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76f5e48941d30253569bc3e9ecec88c5",
     "grade": false,
     "grade_id": "cell-872656208f52d61b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Many machine learning and deep learning Python libraries, such as `sklearn` and `keras`, provide ready-to-use gradient-based optimization algorithms. However, it is instructive to implement our own simplified gradient descent algorithm for learning purposes. In this simple case, we have 100 data points (samples) that are described by only one feature `x`, and a label `y`. \\\n",
    "We need to find the optimal linear predictor `y_pred = weight * x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c13002524902ef5cdb06d8878ef989fc",
     "grade": false,
     "grade_id": "cell-d062ee24a84dc848",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import Python libraries\n",
    "import numpy as np                                 # library for numerical arrays (vectors, matrices, tensors)\n",
    "import matplotlib.pyplot as plt                    # library providing tools for plotting data \n",
    "from sklearn import preprocessing                  # function for pre-processing input data\n",
    "from sklearn.linear_model import LinearRegression  # sklearn class for fitting linear predictor \n",
    "from sklearn.datasets import make_regression       # function to generate a random regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e34ea9bbeb313f1920887cb5a46b58d7",
     "grade": false,
     "grade_id": "cell-7903eaf0c4e0eb78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 2.1.</b> GRADIENT DESCENT. </h3> [2 points]\n",
    "        \n",
    "Your task is to implement the gradient descent algorithm for a dataset of size $m$, where $i$th datapoint is characterized by only one feature ${x}^{(i)}$. Feature values for all $m$ data points are stored in a feature matrix $\\mathbf{X}$, numpy array of shape (m,1). Labels of datapoints are stored in a label vector $\\mathbf{y}$, numpy array of shape (m,1).\n",
    "\n",
    "Implement GD algorithm in two steps:\n",
    "    \n",
    "- implement gradient \"step\" or one iteration of GD in Python function `gradient_step_onefeature()`\n",
    "- define Python function `GD_onefeature()`, which uses `gradient_step_onefeature()` and iterates the gradient step epochs times.\n",
    "  \n",
    "**Implementation note.**\n",
    "\n",
    "You can use for-loop to compute training loss and gradient for each datapoint, but we advise to use vectorized implementation where you use NumPy arrays $\\mathbf{X}$ and $\\mathbf{y}$ and matrix multiplication operation for computing predictions and gradient.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "808af91ca9e33b746421da72ac8b186a",
     "grade": false,
     "grade_id": "cell-d23608ab77205d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "\n",
    "To implement the `gradient_step_onefeature()` function you will need to perform following steps:\n",
    "    \n",
    "1. Compute predictions for all data points, given the current weight $w$, and store predictions in numpy array `y_hat` of shape (m,1).\\\n",
    "The prediction for $i$th data point is:\n",
    "\\begin{align} \n",
    "\\hat{y}^{(i)} =w{x}^{(i)}\n",
    "\\end{align} \n",
    "\n",
    "\n",
    "2. Compute the MSE training loss, given the true labels $y^{(i)}$ and the predictions $\\hat{y}^{(i)}$ for $m$ data points. Store the result in a variable `MSE` (scalar).\\\n",
    "The training loss for $i$th data point is:\n",
    "\\begin{align} \n",
    "L\\big(y^{(i)}, \\hat{y}^{(i)} \\big)= \\big(y^{(i)} - \\hat{y}^{(i)} \\big)^{2}\n",
    "\\end{align}    \n",
    "To obtain MSE loss over $m$ data points you need to compute **average** of training losses over $m$ datapoints.\n",
    "\n",
    "\n",
    "3. Compute the **average** gradient of the loss function $f\\big(w \\big)$.\\\n",
    "The gradient of loss function for $i$th data point given current weight ${w}^{(k)}$ is:\n",
    "\\begin{align} \n",
    "\\nabla f\\big(w^{(k)} \\big)= - 2x\\big(y^{(i)} - \\hat{y}^{(i)} \\big)\n",
    "\\end{align} \n",
    "or\n",
    "\\begin{align} \n",
    "\\nabla f\\big(w^{(k)} \\big)= - 2x\\big(y^{(i)} - w^{(k)}{x}^{(i)} \\big)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "4. Update the weight - change the weight values to the opposite direction from the gradient \n",
    "    \n",
    "\\begin{align} \n",
    "w^{(k+1)}= w^{(k)} - \\alpha\\nabla f\\big(w^{(k)} \\big)\n",
    "\\end{align}   \n",
    "     </div>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8ccfb5ed1a1a0db90ed800504b05fce",
     "grade": false,
     "grade_id": "cell-788861738364f21c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_step_onefeature(X, y, weight, lrate):\n",
    "    '''\n",
    "    Function for performing one gradient descent step for a linear predictor and MSE as a loss function.\n",
    "\n",
    "    Inputs:\n",
    "    - X: numpy array of shape (m,1), feature matrix\n",
    "    - y: numpy array of shape (m,1), label vector \n",
    "    - weight: float number, model's weight, used to compute the prediction\n",
    "    - lrate: float number, learning rate or step size; a coefficient alpha used during weight update\n",
    "\n",
    "    Outputs:\n",
    "    - weight: float number, updated weight value\n",
    "    - MSE: float number, MSE loss on the dataset (X, y)   \n",
    "    '''\n",
    "\n",
    "    # Compute predictions\n",
    "    y_hat = X * weight\n",
    "\n",
    "    # Compute MSE loss\n",
    "    MSE = np.mean((y - y_hat)**2)\n",
    "\n",
    "    # Compute the average gradient of the loss function\n",
    "    grad_w = -2 * np.mean(X * (y - y_hat))\n",
    "\n",
    "    # Update the weights\n",
    "    weight = weight - lrate * grad_w\n",
    "\n",
    "    return weight, MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16f2e0d7cdeedb8580e314ba9884dabe",
     "grade": false,
     "grade_id": "cell-8ee03d326c042553",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "X, y, weight = np.random.rand(10,1), np.random.rand(10,1), np.random.rand()    \n",
    "w, MSE = gradient_step_onefeature(X, y, weight, lrate=0.01)\n",
    "\n",
    "assert isinstance(w, float), \"Output `weight` should be a float!\"\n",
    "assert isinstance(MSE, float), \"Output `MSE` should be a float!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c99c4be994d86e7d6dab7212f0365864",
     "grade": false,
     "grade_id": "cell-bae380ea380e69a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"main-container\">\n",
       "    <div class=\"title-container\">\n",
       "        <h1 class=\"title\">Congratulations!</h1>\n",
       "        <h3 class=\"subtitle\">You have passed the test cases.</h3>\n",
       "\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 1</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.22199317]\n",
       " [0.87073231]]</p>\n",
       " <p>y = [[0.20671916]\n",
       " [0.91861091]]</p>\n",
       " <p>lrate = 0.5</p>\n",
       " <p>weight = 0.01</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.4288399389984786, 0.43487223311756806)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.4288399389984786, 0.43487223311756806)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 2</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.61174386]\n",
       " [0.76590786]\n",
       " [0.51841799]\n",
       " [0.2968005 ]]</p>\n",
       " <p>y = [[0.18772123]\n",
       " [0.08074127]\n",
       " [0.7384403 ]\n",
       " [0.44130922]]</p>\n",
       " <p>lrate = 1.0</p>\n",
       " <p>weight = 1.0</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.6863930332906611, 0.17963525849230216)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.6863930332906611, 0.17963525849230216)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 3</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.27408646]\n",
       " [0.41423502]\n",
       " [0.29607993]\n",
       " [0.62878791]\n",
       " [0.57983781]\n",
       " [0.5999292 ]]</p>\n",
       " <p>y = [[0.26581912]\n",
       " [0.28468588]\n",
       " [0.25358821]\n",
       " [0.32756395]\n",
       " [0.1441643 ]\n",
       " [0.16561286]]</p>\n",
       " <p>lrate = 1.5</p>\n",
       " <p>weight = 0.01</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.3302620145732867, 0.05976147812656014)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.3302620145732867, 0.05976147812656014)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 4</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.99598891]\n",
       " [0.47740172]\n",
       " [0.68491561]\n",
       " [0.84337526]\n",
       " [0.61612483]\n",
       " [0.56318751]\n",
       " [0.36816219]\n",
       " [0.6909312 ]]</p>\n",
       " <p>y = [[0.81616768]\n",
       " [0.92564349]\n",
       " [0.00573506]\n",
       " [0.72055957]\n",
       " [0.66705914]\n",
       " [0.19976541]\n",
       " [0.69666493]\n",
       " [0.57541371]]</p>\n",
       " <p>lrate = 1.0</p>\n",
       " <p>weight = 1.0</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.833491278924551, 0.1206943173975559)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.833491278924551, 0.1206943173975559)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 5</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.3989448 ]\n",
       " [0.81373248]\n",
       " [0.5464565 ]\n",
       " [0.77085409]\n",
       " [0.48493107]\n",
       " [0.02911156]\n",
       " [0.08652569]\n",
       " [0.11145381]\n",
       " [0.25124511]\n",
       " [0.96491529]]</p>\n",
       " <p>y = [[0.63176605]\n",
       " [0.8166602 ]\n",
       " [0.566082  ]\n",
       " [0.63535621]\n",
       " [0.81190239]\n",
       " [0.92668262]\n",
       " [0.91262676]\n",
       " [0.82481072]\n",
       " [0.09420273]\n",
       " [0.36104842]]</p>\n",
       " <p>lrate = 0.5</p>\n",
       " <p>weight = 0.1</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.3382900967401642, 0.44543225031718564)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.3382900967401642, 0.44543225031718564)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your solution \n",
    "from round02 import test_gradient_step_one_feature\n",
    "\n",
    "test_gradient_step_one_feature(gradient_step_onefeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0bcc19e561a125e0a5ca90ef356ffbd",
     "grade": true,
     "grade_id": "cell-850ad195e8f09930",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f276ebd59b1ee6fd24966f7b27c18c58",
     "grade": false,
     "grade_id": "cell-988a0c530bc19ce6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's use the `gradient_step_onefeature()` function and define a new function, `GD_onefeature()`, which will repeat the gradient step for a fixed amount of times (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "123d7f19cc4a361a99689b35adb6a624",
     "grade": false,
     "grade_id": "cell-4a3575bb4c4ff610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def GD_onefeature(X, y, epochs, lrate):  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function to perform GD for linear predictor and MSE as loss function.\n",
    "    The helper function `gradient_step_onefeature()` performs gradient step for a dataset of size `m`, \n",
    "    where each datapoint has only one feature. \n",
    "    \n",
    "    The inputs to the function are:\n",
    "    \n",
    "    :X       : numpy array of shape (m,1), feature matrix\n",
    "    :y       : numpy array of shape (m,1), label vector \n",
    "    :epochs  : integer number, number of epochs \n",
    "    :lrate   : float number, learning rate or step size; a coefficient alpha used during weight update\n",
    "\n",
    "    The outputs of the function are:\n",
    "    \n",
    "    :weights : list of float numbers, weights returned by `gradient_step_onefeature()` function on each epoch\n",
    "    :losses  : list of float numbers, MSE loss returned by `gradient_step_onefeature()` function on each epoch  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # initialize weight value randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand()    \n",
    "    # create lists to store the loss and weight values \n",
    "    losses = []\n",
    "    weights = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "        # run the gradient step for the whole data set\n",
    "        weight, MSE = gradient_step_onefeature(X,y,weight,lrate)\n",
    "        # store current weight and training loss \n",
    "        weights.append(weight)\n",
    "        losses.append(MSE)\n",
    "                       \n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d4c7169d9763df875589e02c7a8db24",
     "grade": false,
     "grade_id": "cell-f786b3942c413385",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code snippet below uses the sklearn.dataset `make_regression` function to generate data points with features and labels. The feature and label values are obtained from random generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19005876de64c57e4f991841e841cc76",
     "grade": false,
     "grade_id": "cell-663b4c32a2e5d018",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix X (n_samples, n_features):  (100, 1)\n",
      "Shape of label vector y (n_samples, 1):  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# generate dataset for regression problem\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42) \n",
    "y = y.reshape(-1,1)\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "print(\"Shape of feature matrix X (n_samples, n_features): \", X.shape)\n",
    "print(\"Shape of label vector y (n_samples, 1): \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd53a65d57ef9fe878f0ef01c4ad2653",
     "grade": false,
     "grade_id": "cell-c9f775200ef813cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We used the sklearn `preprocessing` module ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn-preprocessing-scale)) to scale our features `x`. Learn [here](https://www.youtube.com/watch?v=r5E2X1JdHAU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=20), why it is useful to normalize the data when applying the gradient descent algorithm. \n",
    "\n",
    "Below we use our simple GD algorithm and plot results in two graphs: Loss vs weight values and Loss vs epochs. You can see that weights are converging to the optimum value, resulting in a steep decrease of training loss within first 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8079ec4c933f25705f22c8b04922a7d7",
     "grade": false,
     "grade_id": "cell-2c531e22be633745",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Expected plot</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "    <img src=\"../../../coursedata/SGD/expected_plot/GD_onefeature_plot.png\" width=800/>\n",
    "    </div>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90db2ab2e9dc9b42575bfbab5747240b",
     "grade": false,
     "grade_id": "cell-b4ce18fd464b6ab3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set epoches and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.1\n",
    "\n",
    "# call GD_onefeature() function \n",
    "weights, losses = GD_onefeature(X, y, epochs, lrate)\n",
    "\n",
    "# plot loss and weight values\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,3), sharey=True)\n",
    "\n",
    "# Loss vs weights plot\n",
    "ax[0].scatter(weights, losses)\n",
    "ax[0].set_xlabel(\"weight\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "\n",
    "# Loss vs epoch plot\n",
    "ax[1].plot(range(epochs), losses)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bee7ed6eaae8d47ddba609b0cdec5536",
     "grade": false,
     "grade_id": "cell-d745d2e5ff5696e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "   <h3><b>STUDENT TASK 2.2.</b> GRADIENT DESCENT - LEARNING RATE. </h3> [2 points]\n",
    "\n",
    "Your task is to try out different learning rates for the GD implementation provided in the function `GD_onefeature()`. For each value of the learning rates in the list `lrates` run the function `GD_onefeature()` that returns the sequence of weights $w^{(1)},w^{(2)},\\ldots$ and corresponding loss values $f\\big(w^{(1)}\\big),f\\big(w^{(2)}\\big)$. Store these sequences in the lists `weights_list` and `loss_list`, respectively. For each learning rate, depict the trajectory of GD given by the points $\\big(w^{(k)},f\\big(w^{(k)}\\big)\\big)$, index by the GD iteration number $k$, in a plot with horizontal axis representing the weight values and vertical axis representing the loss function values (see plot below for reference). <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80d34ce1967e85be87a32e344188f6bd",
     "grade": false,
     "grade_id": "cell-0f7d75b7846520e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Expected plot</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "    <img src=\"../../../coursedata/SGD/expected_plot/STlrate.png\" width=460/>\n",
    "    </div>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "664a65d93b77a31074d2f5005c088216",
     "grade": false,
     "grade_id": "cell-811a8a4d8714a5ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Given GD_onefeature() function\n",
    "\n",
    "def gradient_step_onefeature(X, y, weight, lrate):\n",
    "    y_hat = X * weight\n",
    "    MSE = np.mean((y - y_hat)**2)\n",
    "    grad_w = -2 * np.mean(X * (y - y_hat))\n",
    "    weight = weight - lrate * grad_w\n",
    "    return weight, MSE\n",
    "\n",
    "def GD_onefeature(X, y, initial_weight, lrate, epochs):\n",
    "    weights = np.zeros(epochs + 1)\n",
    "    losses = np.zeros(epochs + 1)\n",
    "    weight = initial_weight\n",
    "    weights[0] = weight\n",
    "    for epoch in range(epochs):\n",
    "        weight, MSE = gradient_step_onefeature(X, y, weight, lrate)\n",
    "        weights[epoch + 1] = weight\n",
    "        losses[epoch] = MSE\n",
    "    return weights, losses\n",
    "\n",
    "# Given parameters\n",
    "epochs = 100\n",
    "lrates = [0.001, 0.01, 0.1, 0.9]\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "\n",
    "# Iterate over learning rates\n",
    "for lrate in lrates:\n",
    "    # Generate synthetic data for demonstration\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "    # Standardize X (for stability)\n",
    "    X = (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "    # Run GD for the current learning rate\n",
    "    weights, losses = GD_onefeature(X, y, initial_weight=0, lrate=lrate, epochs=epochs)\n",
    "\n",
    "    # Append results to lists\n",
    "    weights_list.append(weights)\n",
    "    loss_list.append(losses)\n",
    "\n",
    "# Plot results\n",
    "for i, lrate in enumerate(lrates):\n",
    "    plt.plot(weights_list[i], loss_list[i], label=f\"lrate {lrate}\")\n",
    "\n",
    "plt.xlabel(\"Weight\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df53aef81d8a6bb0e14941aaac6174fb",
     "grade": false,
     "grade_id": "cell-aa358a025eb64e94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Length of list `weights_list[0]` should be equal to epoch number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sanity checks\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weights_list)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(lrates), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of list `weights_list` should be equal to length of `lrates`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weights_list[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m==\u001b[39mepochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of list `weights_list[0]` should be equal to epoch number\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(loss_list)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(lrates), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of list `loss_list` should be equal to length of `lrates`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(loss_list[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m==\u001b[39mepochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of list `loss_list[0]` should be equal to epoch number\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Length of list `weights_list[0]` should be equal to epoch number"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "assert len(weights_list)==len(lrates), \"Length of list `weights_list` should be equal to length of `lrates`\"\n",
    "assert len(weights_list[0])==epochs, \"Length of list `weights_list[0]` should be equal to epoch number\"\n",
    "assert len(loss_list)==len(lrates), \"Length of list `loss_list` should be equal to length of `lrates`\"\n",
    "assert len(loss_list[0])==epochs, \"Length of list `loss_list[0]` should be equal to epoch number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e359448a35587078d6beeab740568d97",
     "grade": true,
     "grade_id": "cell-6bffb19cd2e7f648",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f011875b1492f7e34df7b7b52e073031",
     "grade": true,
     "grade_id": "cell-13c46b047c6751a7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e71d634aca6fa4bb87b555c049292cfc",
     "grade": false,
     "grade_id": "cell-8e6ed71ea6242596",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on the graph, which learning rate values do you think are suboptimal? What will happen if learning rate `lrate= 1` is used?\n",
    "\n",
    "Below we plot data points and linear predictors with weights computed at 1st, 50th and 100th epochs for each learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8025d1a6b73cc01f4a88ee5df94262dc",
     "grade": false,
     "grade_id": "cell-3a7b8b2316971726",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,4, sharey=True, figsize=(15,4))\n",
    "\n",
    "for i in range(len(lrates)):\n",
    "        x_grid = np.linspace(X.min(),X.max(),100)     # x-axis values for plotting\n",
    "        y_pred_1 = weights_list[i][0]*x_grid          # get weight computed at epoch 1 and compute predictions\n",
    "        y_pred_50 = weights_list[i][49]*x_grid        # epoch 50\n",
    "        y_pred_100 = weights_list[i][99]*x_grid       # epoch 100\n",
    "        \n",
    "        ax[i].scatter(X,y) # plot data points\n",
    "        ax[i].plot(x_grid, y_pred_1, label=\"epoch 1\", c='k')      # plot predictor with weight at epoch 1\n",
    "        ax[i].plot(x_grid, y_pred_50, label=\"epoch 50\", c='g')    # plot predictor with weight at epoch 50\n",
    "        ax[i].plot(x_grid, y_pred_100, label=\"epoch 100\", c='r')  # plot predictor with weight at epoch 100\n",
    "        \n",
    "        # remove top and right subplot's frames \n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "        # set subplot's title\n",
    "        ax[i].set_title(\"lrate = \"+str(lrates[i]), fontsize=18)\n",
    "\n",
    "ax[3].legend()\n",
    "plt.ylim(-150,150)      \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "047a1fa074dfa85e730fe4f82e970cab",
     "grade": false,
     "grade_id": "cell-21024b817ac1e1a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St3'></a>\n",
    "<div class=\" alert alert-success\">\n",
    "    <h3><b>STUDENT TASK 2.3. (OPTIONAL, not graded) . </b> GRADIENT DESCENT. </h3> \n",
    "    \n",
    "This is an <strong>optional (not graded!)</strong> task where you will try vectorized implementation of GD for data points with many features. You will need the knowledge of matrix multiplication for completing the task.\n",
    "    \n",
    "    \n",
    "Write a gradient descent implementation for a dataset of size $m$, where $i$th datapoint is characterized by $n$ features stored in feature vector $\\mathbf{x}^{(i)}$.\n",
    "\n",
    "The feature vectors of each datapoint are stored in a matrix $X$ of shape $(m,n)$. The true labels of datapoints are stored in the label vector $\\mathbf{y}$ of shape $(m,1)$. \n",
    "\n",
    "You need to implement similar steps as previously, but in this case the loss function is:\n",
    "\n",
    "\\begin{align} \n",
    "f\\big( \\mathbf{w}\\big)= (1/m) \\sum_{i=1}^{m}\\big(y^{(i)} - \\mathbf{w}^{T} \\mathbf{x}^{(i)} \\big)^2\n",
    "\\end{align}\n",
    "    \n",
    "    \n",
    "and the gradient:\n",
    "    \n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big)\n",
    "\\end{align}\n",
    "    \n",
    "where $\\mathbf{x}^{(i)}$ and  $\\mathbf{w}$ are arrays of shape $(n,1)$ and $n$ is a number of features.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a29bf2c805ae9e91a9fc53c6ca9bfcdd",
     "grade": false,
     "grade_id": "cell-94ac1f19a0bf58bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "\n",
    "To implement the vectorized GD, you can use following formulas:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "  \\mathbf{\\hat{y}} &= X \\times \\mathbf{w} \\\\\n",
    "  \\\\\n",
    "  MSE &= \\frac{1}{m}  \\Sigma_{i=1}^m{(\\mathbf{y} - \\mathbf{\\hat{y}})^2} \\\\\n",
    "  \\\\\n",
    "  \\nabla f\\big( \\mathbf{w} \\big) &= - \\frac{2}{m} X^T \\times (\\mathbf{y} - \\mathbf{\\hat{y}}) \\\\\n",
    "  \\\\\n",
    "  \\mathbf{w} &= \\mathbf{w} - \\alpha \\nabla f\\big( \\mathbf{w} \\big)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where: <br>\n",
    " $\\times$ represents the matrix multiplication (`@` operator in numpy) and $T$ is a transpose operator.\n",
    "     </div>    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abbadc2f84838d775628a385eb6b9157",
     "grade": false,
     "grade_id": "cell-fe84e9067c0aa837",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_step(X, y, weight, lrate):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function for performing gradient descent step for linear predictor and MSE as a loss function.\n",
    "    \n",
    "    The inputs to the function are:\n",
    "    \n",
    "    :X:      numpy array of shape (m,n), feature matrix\n",
    "    :y:      numpy array of shape (m,1), label vector \n",
    "    :weight: numpy array of shape (n,1), weight vector\n",
    "    :lrate:  float number, learning rate or step size; a coefficient alpha used during weight update\n",
    "\n",
    "    The outputs of the function are:\n",
    "    \n",
    "    :weight: numpy array of shape (n,1), updated weight vector\n",
    "    :MSE:    float number, MSE loss on the dataset (X,y)   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "    \n",
    "    # performing Gradient Step:\n",
    "\n",
    "    # 1. Compute predictions, given the feature matrix X of shape (m,n) and weight vector w of shape (n,1).\n",
    "    #    Predictions should be stored in an array `y_hat` of shape (m,1).\n",
    "    # y_hat = ...\n",
    "    \n",
    "    # 2. compute MSE loss\n",
    "    # MSE = ...\n",
    "    \n",
    "    # 3. compute average gradient of loss function\n",
    "    # gradient = ...\n",
    "    \n",
    "    # 4. update the weights\n",
    "    # weight = ...\n",
    "    \n",
    "    return weight, MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96c07b6e23cd04afe79aa96be2351902",
     "grade": false,
     "grade_id": "cell-f7aa818e7be52c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test your solution \n",
    "from round02 import test_gradient_step\n",
    "\n",
    "test_gradient_step(gradient_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da51a8f415fc9cd0d2580293dda2a4ba",
     "grade": false,
     "grade_id": "cell-47e93370eaaec0a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's use the completed `gradient_step()` function and define the `GD()` function that will repeat the gradient step for fixed amounts of times (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c309471782aab177166746006f1b0fd9",
     "grade": false,
     "grade_id": "cell-6e70b0a0a2d5117d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def GD(X, y, epochs, lrate):  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function to perform GD for linear predictor and MSE as loss function.\n",
    "    The helper function `gradient_step` performs gradient step for dataset of size `m`, \n",
    "    where each datapoint has `n` features. \n",
    "    \n",
    "    The inputs to the function are:\n",
    "    \n",
    "    :X:      numpy array of shape (m,n), feature matrix\n",
    "    :y:      numpy array of shape (m,1), label vector \n",
    "    :epochs: integer number, number of epochs \n",
    "    :lrate:  float number, learning rate or step size; a coefficient alpha used during weight update\n",
    "\n",
    "    \n",
    "    The outputs of the function are:\n",
    "    \n",
    "    :weights: list of numpy arrays, weights returned by `gradient_step()` function on each epoch\n",
    "    :losses:  list of float numbers, MSE loss returned by `gradient_step()` function on each epoch  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # initialize the weight vector randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand(X.shape[1],1)    \n",
    "    # create a list to store the loss values \n",
    "    weights = []\n",
    "    losses = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "        # run the gradient step for the whole dataset\n",
    "        weight, MSE = gradient_step(X, y, weight, lrate)\n",
    "        # store the MSE loss of each batch of each epoch\n",
    "        weights.append(weight)\n",
    "        losses.append(MSE)\n",
    "                       \n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "347c5cbcd63ddad5bb58e49591c103e5",
     "grade": false,
     "grade_id": "cell-aaff04830999aeb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# generate a dataset for a regression problem\n",
    "# in this case we will set number of features to four\n",
    "\n",
    "X2, y2 = make_regression(n_samples=100, n_features=4, noise=20, random_state=42) \n",
    "y2 = y2.reshape(-1,1)\n",
    "\n",
    "X2 = preprocessing.scale(X2)\n",
    "\n",
    "print(\"Shape of feature matrix X2 (n_samples, n_features): \", X2.shape)\n",
    "print(\"Shape of label vector y2 (n_samples, 1): \", y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb536089805501c300015ae946a837af",
     "grade": false,
     "grade_id": "cell-5886c6547293019b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Expected plot</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "    <img src=\"../../../coursedata/SGD/expected_plot/GD_plot.png\" width=600/>\n",
    "    </div>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "539bf211db591288beb022cbef3fa961",
     "grade": false,
     "grade_id": "cell-1d16b774ab7239a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set epoch and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.1\n",
    "\n",
    "# store results\n",
    "(weights, losses) = GD(X2, y2, epochs, lrate)\n",
    "\n",
    "# print the cost function\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "ax.set_ylabel('Loss', fontsize=16)\n",
    "ax.set_xlabel('epochs', fontsize=16)\n",
    "ax.plot(range(epochs), losses, 'b.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d08c0439808a635f95fb53bbd1296e5",
     "grade": false,
     "grade_id": "cell-dfd4b62673e3f59b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 2.4.</b> Compare your implementation with sklearn `LinearRegression` class. </h3>\n",
    "    [0.25 points]\n",
    "    \n",
    "We can compare the optimal weights learned by our simple algorithm and the optimal weight calculated by Sklearn's <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html>LinearRegression</a> class. This class does not use iterative gradient-based algorithms but rather calculate the optimal weight analytically <a href=\"https://en.wikipedia.org/wiki/Linear_least_squares\">learn more here</a>.\n",
    "\n",
    "Retrieve weight returned by `GD_onefeature()` function on the last epoch. Store it in a variable `gd_weight`.\n",
    "Note if weight returned by `GD_onefeature()` is similar to the optimal weight returned by sklearn `LinearRegression()` class. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d15f8eca354a5a7b72af9e730623fb0",
     "grade": false,
     "grade_id": "cell-768f3b5086d9d2bc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set epoch and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.1\n",
    "\n",
    "# create a linear regression model \n",
    "reg = LinearRegression(fit_intercept=False) \n",
    "# fit the linear regression model\n",
    "reg.fit(X, y)\n",
    "# print the optimal coefficients\n",
    "print(f'Optimal weights calculated by the LinearRegression model: {reg.coef_.reshape(-1,)}')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# retrieve weight values returned by `GD_onefeature()` on the LAST epoch\n",
    "#gd_weight = ...\n",
    "\n",
    "print(f'{\" \"*9} Optimal weights calculated by the GD algorithm: {gd_weight.reshape(-1,)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad9952ade102e5a533a5678620515b01",
     "grade": false,
     "grade_id": "cell-87e4e9c976e542b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "from numpy.testing import assert_almost_equal\n",
    "assert_almost_equal(reg.coef_.reshape(-1,), gd_weight.reshape(-1,), err_msg=\"reg.coef_ and gd_weight should be equal! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92cd84083c42f0989d931559f616eab1",
     "grade": true,
     "grade_id": "cell-d268e98b5be767f7",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61f30fbe346aece2ff663779dd2d4244",
     "grade": false,
     "grade_id": "cell-5b81fb9fdb99f93d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With correct implementation of `gradient_step_onefeature()` you should see that optimal weight value returned by our algorithm and `LinearRegression` are close (equal to 7 decimal places). Thus, our simple gradient descent algorithm does not deviate too much from the `Sklearn` implementation of linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "571ce7c9bdffa8fb3e40e5db614716b5",
     "grade": false,
     "grade_id": "cell-bbaa97e8321c59a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St5'></a>\n",
    "<div class=\" alert alert-success\">\n",
    "     <h3><b>STUDENT TASK 2.5. (OPTIONAL, not graded). </b> STOPPING CRITERIA. </h3>\n",
    "    \n",
    "Your task is to modify <code>GD_onefeature()</code> or <code>GD()</code> code and introduce stopping criteria other than a fixed number of iterations (epochs). You will need to compare loss values returned on the last two epochs and stop GD when the loss will stop decreasing significantly (e.g. the loss decrease is less than 1e-6). In addition, introduce a maximum number of epochs GD can perform, in order to prevent GD from running for a long time. The function should return weight(s) and loss value on the last iteration, and the number of epochs performed.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69d102e2514604aae066731f73d0cf9f",
     "grade": false,
     "grade_id": "cell-38261cb510dfb699",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    " <div class=\"summary-content\">\n",
    "     <ul>\n",
    " <li> You can use for-loop to iterate epoch times (maximum number of iterations) and if loop to check if the stopping condition(s) is true.</li>\n",
    " <li> You can use `math.isclose()` function to compare 2 values to desired precision.</li>\n",
    "     </ul>\n",
    "  </div>    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8962b8844b1f9fb3388a94e9b8f30c8",
     "grade": false,
     "grade_id": "cell-7bd15f464106855b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def GD_stop(X, y, epochs, lrate):  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function for performing GD for linear predictor and MSE as loss function.\n",
    "    The helper function `gradient_step_onefeature()` performs gradient step for dataset of size `m`, \n",
    "    where each datapoint has one feature. \n",
    "    \n",
    "    The inputs to the function are:\n",
    "    \n",
    "    :X:      numpy array of shape (m,n), feature matrix\n",
    "    :y:      numpy array of shape (m,1), label vector \n",
    "    :epochs: integer number, number of epochs \n",
    "    :lrate:  float number, learning rate or step size; a coefficient alpha used during weight update\n",
    "\n",
    "    \n",
    "    The outputs of the function are:\n",
    "    \n",
    "    :weights: list of numpy arrays, weights returned by `gradient_step()` function on each epoch\n",
    "    :losses:  list of float numbers, MSE loss returned by `gradient_step()` function on each epoch  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "    return weight, loss, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02642b99e30362b431a3fdbcc881b1dc",
     "grade": false,
     "grade_id": "cell-4f78dace1c91a426",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "lrate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "# store the results\n",
    "(weight, loss, iterations) = GD_stop(X, y, epochs, lrate)\n",
    "\n",
    "print(f'Number of epochs: {iterations}\\nLoss: {loss}\\nWeights:{weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95babe2691885c7dc1cfe322b400d3c7",
     "grade": false,
     "grade_id": "cell-d1ebe1415256d4b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note if algorithm stopped earlier than epochs set. Try to change tolerance parameters in `math.isclose()` and see if there is a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec72f8151fc6058dd8f324927dd910d8",
     "grade": false,
     "grade_id": "cell-9a0211637fcab536",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"wrap-up\">\n",
    "    <div class=\"wrap-up-title\">Wrap up - Gradient Descent</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "        <ul>\n",
    "          <li>Gradient Descent (GD) is one of the most popular optimization methods used in the training of artificial neural networks.</li>\n",
    "          <li>GD is an iterative algorithm used to find optimal parameters (weights and biases) of the model (ANN).</li>\n",
    "          <li>GD makes small steps in the opposite direction of the gradient, which is also the direction of the steepest descent.</li>\n",
    "          <li>Learning rate of GD will define how big are steps made in order to reach the minimum of the loss function.</li>\n",
    "          <li>If the learning rate is too small, it will take a long time for GD to learn optimal weights. If the learning rate is too high, GD parameters update will overshoot the minimum of the loss function and loss values will fluctuate around the minimum, or in the worst case, GD may diverge.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7916f4e51875685505280664d6d0d8b9",
     "grade": false,
     "grade_id": "cell-9e201cd34a35a933",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Stochastic Gradient Descent \n",
    "\n",
    "Despite its conceptual simplicity, GD might not be practical in ML applications involving massive amounts of data. Consider image classification where state-of-the-art deep learning methods are trained on billions of images. The challenge in using GD for such big data applications is the computational complexity (time and memory resources) of computing the gradient $\\nabla f\\big( \\mathbf{w}^{(k)} \\big)$ of the loss function at the current estimate $\\mathbf{w}^{(k)}$. \n",
    "\n",
    "Let us have a closer look at the computation of the gradient for the special case of linear predictor maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. In this case, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The difficulty is that the summation involves all $m$ data points that form the training set. Thus, we might need to sum over billions of data points. Moreover, the data points might be stored decentralized all over the internet (in the \"cloud\"). A single iteration of GD might then simply take too long.\n",
    "\n",
    "In order to avoid the computational burden of computing the gradient, **stochastic GD (SGD)** approximates the gradient by using only a small subset (**batch**) of training data points. SGD is obtained from GD by replacing the exact gradient step with a noisy gradient update: \n",
    "\n",
    "$$\\mbox{(Noisy Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha^{(k)}}_{\\mbox{step size}}\\mathbf{g}^{(k)} \\mbox{ with } \\mathbf{g}^{(k)} \\approx \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "\n",
    "Note that we now use a varying step-size $\\alpha^{(k)}$ that changes along with the iterations. This is necessary in order to avoid accumulation of the gradient noise during the SGD updates. SGD methods typically decrease the learning rate $\\alpha^{(k)}$ as the iterations proceed. \n",
    "\n",
    "The most basic variant of SGD uses a single randomly chosen data point for computing the gradient estimate $\\mathbf{g}^{(k)}$. For the special case of linear predictor maps, we obtain \n",
    "$$ \\mathbf{g}^{(k)}   = -2 \\mathbf{x}^{(I)} \\big(y^{(I)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(I)} \\big) \\big).$$\n",
    "\n",
    "Note that the index $I$ of the data point is chosen randomly and independently for each new iteration $k$. Comparing this gradient estimate with the above formula for the exact gradient, we see that the SGD iteration does not require any summation over the training set. For large training sets, this might yield a significant reduction in computational requirements for SGD compared to GD. \n",
    "\n",
    "Plain SGD and GD can be interpreted as special cases of **mini-batch GD**. Mini-batch GD does not use a single randomly chosen data point to compute the gradient estimate but rather uses several randomly chosen data points that form a batch $\\mathcal{S} = \\big\\{ \\big(\\mathbf{x}^{(i_{1})},y^{(i_{1})} \\big),\\ldots,\\big(\\mathbf{x}^{(i_{S})},y^{(i_{S})} \\big) \\big\\}$ of size $s$. For linear predictor maps, the gradient estimate is computed using the batch via \n",
    "\n",
    "$$ \\mathbf{g}^{(k)}   = -(2/s) \\sum_{\\big(\\mathbf{x},y\\big) \\in \\mathcal{S}} \\mathbf{x} \\big(y - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x} \\big) \\big).$$\n",
    "\n",
    "Each iteration of mini-batch GD uses a different batch of $s$ different data points. A sequence of iterations that uses each data point in one of the batches is referred to as one **epoch** (i.e. batches are disjoint). \n",
    "For example, if the training data set consists of $50$ data points and we use a batch size of $s=10$, then one epoch requires $50/10 = 5$ iterations of mini-batch GD.  \n",
    "\n",
    "\n",
    "To recap:\n",
    "\n",
    "- **Sample**: one element of a dataset.\\\n",
    "  Example: one image is a sample in a convolutional network\\\n",
    "  Example: one audio file is a sample for a speech recognition model\n",
    "  \n",
    "  \n",
    "- **Batch**: a set $\\mathcal{S}$ of size $s$. The samples in a batch are processed independently, in parallel. During training, a batch results only in one update (gradient step) of the model.\\\n",
    "A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).\n",
    "\n",
    "\n",
    "- **Epoch**: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n",
    "\n",
    "\n",
    "\n",
    "Important special cases of GD are obtained for certain choices of the **batch size**: \n",
    "\n",
    "- GD (also called batch GD or vanilla GD, batch size = size of dataset)\n",
    "- Mini-batch GD (1 < batch size < whole dataset)\n",
    "- SGD (batch size = 1)\n",
    "\n",
    "Note that the computational complexity of one iteration of mini-batch GD depends only on the batch size. Thus, \n",
    "for a fixed batch size (e.g. $s=128$) the complexity (runtime) of a single iteration becomes independent of the total number of training data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c1b17d34e45485f4a284f1655ae67c7",
     "grade": false,
     "grade_id": "cell-df68d677ce3ac05d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3><b>Check yourself: </b>Batch and Epochs.</h3>\n",
    "\n",
    "Given dataset of size 1000, batch size 100 and 5 epochs, how many iterations of gradient step we need to do?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8fa2dfc2384e018c158d407d221ef6e",
     "grade": false,
     "grade_id": "cell-6993aa1fffa90529",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary><span class=\"summary-title\">Solution</span></summary>\n",
    "    <div class=\"summary-content\">\n",
    "        \n",
    "\n",
    "- Each iteration of mini-batch GD uses a batch of size 100 to compute the gradient estimate and update model's weights. \n",
    "- To go through whole dataset we need to do 1000 / 100 = 10 iterations of GD step per one epoch.\n",
    "- Training a model for 5 epochs, means that we iterate through  whole dataset 5 times, which give us 5 * 10 iterations = 50 iterations in total.\n",
    "</div> \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90486d14b93d89672afa940eded30f80",
     "grade": false,
     "grade_id": "cell-ec008639e2259829",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St6'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 2.6.</b> STOCHASTIC GRADIENT DESCENT.</h3> [2 points]\n",
    "    \n",
    "In this task, we modify our GD algorithm (function `GD_onefeature()`) by providing and training the dataset in batches. You will need to complete <code>minibatchGD()</code> function, according to the instructions given below. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1bb3f0d0e4099c12a040c29ad0a124d",
     "grade": false,
     "grade_id": "cell-c2a00d5a26465f7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before implementing the SGD algorithm, we need to write a helper function `batch()` that divides the dataset into small batches. We will provide the feature vector or matrix `X` and label vector `y` as an input to the function. We also need to define the parameter `batch_size` which is the number of data points used for a single batch. The function `batch()` is a [Python generator function](https://docs.python.org/3/howto/functional.html#generators), meaning that the function can be used in for-loops and will return batches sequentially, one-by-one. Before splitting the dataset into batches, we will randomly shuffle the data. This ensures that every time we call `batch()` we obtain a batch having similar statistical properties. Loosely speaking, shuffling the dataset before creating the batches makes the individual data points independent and identically distributed (\"i.i.d.\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef8d52d136a1385012e10133435e3407",
     "grade": false,
     "grade_id": "cell-76e1beb18772edb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function for creating mini-batches of the dataset.\n",
    "    \n",
    "    The inputs to the function are:\n",
    "    \n",
    "    :X:          numpy array of shape (m,n), feature matrix\n",
    "    :y:          numpy array of shape (m,1), label vector \n",
    "    :batch_size: sample size of a batch. Note, that the last batch may be smaller than the `batch_size` value.\n",
    "\n",
    "    The outputs of the function are:\n",
    "    \n",
    "    The `yield` statement suspends the function’s execution and sends \n",
    "    a value back to the caller, but retains enough state to enable function to resume where it is left off.\n",
    "    \n",
    "    In this case a value is a current batch (X_perm[i:i + batch_size], y_perm[i:i + batch_size])\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # shuffle data points \n",
    "    # the permutation will randomly re-arrange the order of the numbers\n",
    "    # which will be used as indices to create X and y with data points in different order\n",
    "    np.random.seed(42) # for reproducibility, should NOT be used in real training\n",
    "    p = np.random.permutation(len(y))\n",
    "    X_perm = X[p] \n",
    "    y_perm = y[p]\n",
    "    \n",
    "    # generate batches\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_perm[i:i + batch_size], y_perm[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48ef3806d747f05b7f63118a3da2522c",
     "grade": false,
     "grade_id": "cell-a2a173cbe3bcbe36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"info\">\n",
    "    <div  class=\"info-title\"><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Info</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "        You can test Python generator by using <code>next()</code> function:\n",
    "    </div>\n",
    "    \n",
    "```python\n",
    ">>> gen =  Generator(args)\n",
    ">>> next(gen)\n",
    "```\n",
    "    \n",
    "    or for-loop:\n",
    "    \n",
    "```python   \n",
    ">>> for element in Generator(args):\n",
    "    print(element)    \n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd87b246f7da0f70a1bba8f49ae3a72d",
     "grade": false,
     "grade_id": "cell-1dc3b3cb1fc8cb8e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def minibatchGD(X, y, batch_size, epochs, lrate):  \n",
    "    \n",
    "    # initialize the weight randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand()  \n",
    "    # list to store the loss values \n",
    "    losses = []\n",
    "    # list to store the weight values \n",
    "    weights = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "        # Use another for-loop to iterate batch() generator and access batches one-by-one\n",
    "        # Feed  current batch to `gradient_step_onefeature()` and get weight and loss values\n",
    "        # Save current weight and loss values in corresponding lists `weights` and `losses`\n",
    "      \n",
    "        # one epoch is finished when the algorithm goes through ALL batches\n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b11af52ded7f4f7f9eb5939babc0a5e",
     "grade": false,
     "grade_id": "cell-92dea7187bf6ce97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    " <div class=\"summary-content\">\n",
    "     <a href=\"https://www.geeksforgeeks.org/generators-in-python/\">Here</a> you can find an example of using Python generator with for-loop. Use the same approach to iterate `batch()` generator.\n",
    "  </div>    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cf06d1b272602dc94cb972bddc41dd0",
     "grade": false,
     "grade_id": "cell-fe1f1ab467887505",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "# len(weights) and len(loss) should be:\n",
    "# (number of samples/batch_size)*epochs - 100/50*2=4\n",
    "weights, losses = minibatchGD(X, y, 50, 2, 0.1)\n",
    "\n",
    "assert len(weights)==4, \"weights length is not correct!\"\n",
    "assert len(losses)==4, \"loss length is not correct!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e65f806675560127de78e68af2e2843a",
     "grade": true,
     "grade_id": "cell-a7b46020ff3a7ab3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a21ba9232c5953ba9195c3a6a7181307",
     "grade": true,
     "grade_id": "cell-7781c38310cc5b53",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c873573d522994bfc46d1eb8490b4815",
     "grade": false,
     "grade_id": "cell-a14ca933f21b43e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St7'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 2.7. </b> SGD - BATCH SIZE.</h3> [2 points]\n",
    "    \n",
    "Try `minibatchGD()` function with different values of `batch_size` parameter and plot results.\n",
    "    \n",
    "Note - use feature vector **x** and label vector **y** as input data of  `minibatchGD()` function. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ce2dc28d91e0e3a96ccf3429447597c",
     "grade": false,
     "grade_id": "cell-caa8d0782861291e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's test our SGD implementation and run the algorithm for:\n",
    "\n",
    "- batch sizes = 1 (one data point) **(SGD)**\n",
    "- batch sizes = 10 **(mini-batch GD)**\n",
    "- batch sizes = 100 (entire dataset) **(Batch GD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "588a6a253521e1a50a5170260fc6c0d4",
     "grade": false,
     "grade_id": "cell-d3601de090e616a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "     </summary>\n",
    "     <div class=\"summary-content\">\n",
    "You can use for-loop to iterate list `batch_sizes` and pass these values to `minibatchGD()` function. \n",
    "         \n",
    "Expected plot:\n",
    "         \n",
    "         \n",
    "<img src=\"../../../coursedata/SGD/expected_plot/batch_size_plot.png\" width=500>\n",
    "    </div>\n",
    "</details>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78f7811042fbb969248586b31c432182",
     "grade": false,
     "grade_id": "cell-5dbba44eba46a098",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set epoches and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.02\n",
    "\n",
    "# values to pass to `minibatchGD()` function in foor-loop \n",
    "batch_sizes = [1, 10, 100]\n",
    "# list for storing weights and loss for each batch size (length of both lists=3)\n",
    "# we will use these lists for plotting\n",
    "weights_batches = []\n",
    "loss_batches = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# iterate `batch_sizes` list\n",
    "# for ....\n",
    "#     compute weights & losses for a given batch size value\n",
    "#     append weights & losses to lists weights_batches & loss_batches\n",
    "\n",
    "\n",
    "for batch_size, weights, loss in zip(batch_sizes, weights_batches,loss_batches):\n",
    "    plt.plot(weights, loss, label=\"batch size\"+str(batch_size))\n",
    "    plt.legend()\n",
    "\n",
    "plt.xlabel(\"weight\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96c15632257dfab659cdfe6b117470c8",
     "grade": false,
     "grade_id": "cell-ea33d721e419dbee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the deviations are larger for smaller batch sizes. This makes sense, as the gradient estimates are more accurate when we use more data points in a batch. The larger the batch, the better the gradient approximation and the faster the convergence of GD to the optimum solution (parameters). However, it is also true that the batch will take longer to process and will still result in only one update.\n",
    "\n",
    "Let's use the loss values for each batch size stored in the list `loss_batches` for plotting and will print out learned weights stored in `weights_batches` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a5b202d422839a1407175d38886ff29",
     "grade": false,
     "grade_id": "cell-b16146a51390dad9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "assert len(weights_batches)==len(batch_sizes), \"weights_batches length is not correct!\"\n",
    "assert len(loss_batches)==len(batch_sizes), \"loss_batches length is not correct!\"\n",
    "# below length computed as (number of samples/batch_size)*epochs\n",
    "assert len(loss_batches[0])==10000, \"loss_batches[0] length should be (100/1)*100=10000!\"\n",
    "assert len(loss_batches[1])==1000, \"loss_batches[1] length should be (100/10)*100=1000!\"\n",
    "assert len(loss_batches[2])==100, \"loss_batches[2] length should be (100/100)*100=100!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d4f6926652e144d1dfa20f0fda11448",
     "grade": true,
     "grade_id": "cell-fa4f330c2cb1e60e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55a5a6e50a12b8d8cb3e349b9a4785f0",
     "grade": true,
     "grade_id": "cell-41c59e0f0f2e20fe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5034f40391c6d5fe3d325df88672b28c",
     "grade": false,
     "grade_id": "cell-f507efc6bc25907f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history of the MSE loss inccured during learning\n",
    "batch_size1_loss   = loss_batches[0]\n",
    "batch_size10_loss  = loss_batches[1]\n",
    "batch_size100_loss = loss_batches[2]\n",
    "\n",
    "# let's check that the length of list `loss` is equal to\n",
    "# x.shape[0]/batch_size*epochs\n",
    "\n",
    "print(f\"Total number of iterations = (sample size/batch size)*epochs\")\n",
    "print(f\"\\nEpochs: {epochs}\")\n",
    "print(f\"Sample size: {X.shape[0]}\")\n",
    "print(f\"Batch sizes: 1, 10, 100\")\n",
    "print(f\"Iterations per epoch: {X.shape[0]/1.0:.0f}, {X.shape[0]/10.0:.0f}, {X.shape[0]/100.0:.0f}\")\n",
    "print(f\"Total number of iterations: {len(batch_size1_loss)}, {len(batch_size10_loss)}, {len(batch_size100_loss)}\")\n",
    "\n",
    "# display weights learnt during the SGD\n",
    "print(f\"\\nWeights:\\n\\nSGD with batch size = 1 results in weight w = {weights_batches[0][-1]:.2f}\\\n",
    "                 \\nSGD with batch size = 10 results in weight w = {weights_batches[1][-1]:.2f}\\\n",
    "                 \\nSGD with batch size = 100 results in weight w = {weights_batches[2][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86f0468801ddd9e243ee942ed1f3a2ea",
     "grade": false,
     "grade_id": "cell-197719c66c1ae6fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The results of SGD with different batch sizes should be pretty close. Let's compare our SGD implementation with sklearn [`SGDRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) class implementation. In `SGDRegressor` implementation, the gradient of the loss is estimated each sample at a time (batch size=1 in other words) and the model is updated along the way with a decreasing learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27d3d36ab98ad2068693eef70e4bba3a",
     "grade": false,
     "grade_id": "cell-073a69e706d17886",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "reg = SGDRegressor(fit_intercept = False, max_iter=100, tol=1e-3)\n",
    "reg.fit(X,y.reshape(-1,))\n",
    "\n",
    "reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bcff507a92616b758ec17b1eaa2e350",
     "grade": false,
     "grade_id": "cell-97f489247f04856f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Thus, our simple SGD algorithm does not deviate too much from the `sklearn` implementation. \\\n",
    "Let's plot the loss values for the first 100 iterations incurred during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce26fe7fd6890f027ce7202b6d7effe",
     "grade": false,
     "grade_id": "cell-e755ac6d58a80022",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the figure and axes objects\n",
    "# there will be 3 subplots in one row, the y-axis is shared between subplots\n",
    "fig, axes = plt.subplots(1,3, sharey=True, figsize=(15,5))\n",
    "\n",
    "# create lists of loss values and batch sizes for further iteration in for-loop\n",
    "batch_loss_list = [batch_size1_loss, batch_size10_loss, batch_size100_loss]\n",
    "batch_size      = [1,10,100] \n",
    "\n",
    "for ax, batch_loss, size in zip(axes, batch_loss_list, batch_size):\n",
    "    # plot only first 100 values\n",
    "    ax.plot(np.arange(len(batch_loss[:100])), batch_loss[:100])\n",
    "    # remove top and right subplot's frames \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    # set subplot's title\n",
    "    ax.set_title(\"batch size = \"+str(size), fontsize=18)\n",
    "\n",
    "# set x- and y-axis labels\n",
    "axes[0].set_xlabel('batch #', fontsize=18)\n",
    "axes[0].set_ylabel('Loss', fontsize=18)\n",
    "\n",
    "# display figure\n",
    "plt.ylim(0,10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eafa8fdbae52a06aa8ccb448cb776efb",
     "grade": false,
     "grade_id": "cell-9b626daecc63f8bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The above figure indicates that when using SGD with a small batch size, the loss is not decreasing monotonically but somewhat randomly fluctuating around a long-term decreasing trend. This happens because the weight updates use \"noisy\" estimates of the gradient. The noisy estimate is calculated by an averaging process using the data points in the mini-batch. The smaller the mini-batch size, the fewer data points we use for computing the average. Thus, the gradient noise becomes stronger with smaller batch size. \n",
    "\n",
    "**Note!** In our exercises we used a constant learning rate. In order to avoid the accumulation of the gradient noise while running SGD updates, the learning rate needs to be gradually decreased, e.g. by using diminishing learning rate, thus resulting in a smoother learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b99fff2f4016145428785f307fb8356",
     "grade": false,
     "grade_id": "cell-2a6b165617a3a4b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below are the animations illustrating the training process with GD where the batch size is all data points (upper panel) and where the batch size is 10 data points (lower panel, the current batch marked with red color). In line with the loss plots we created above, mini-batch GD is noisier than batch GD. Although it seems that using plain batch GD is a faster way to reach the minimum of the loss function, in practice when working with large datasets and thousands of parameters (weights) in neural networks, this approach will be slower and more computationally expensive than mini-batch GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "757af3f302c4854f5f44bb4d5c33b5e6",
     "grade": false,
     "grade_id": "cell-2032e57953d2d15e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Illustration of SGD iterations for batch size = 100 (batch covers entire dataset)** \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/minibatchGD1.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "**Illustration of SGD iterations for batch size = 10 datapoints**\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/minibatchGD2.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "During each iteration of SGD, 10 data points are randomly selected to constitute a batch. This batch is used to compute the gradient estimate. The data points in the batch are shown in red. Note that during each iteration, a different set of 10 data points is chosen for the batch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c1b329db94545c0f0bc0249f93f18c2",
     "grade": false,
     "grade_id": "cell-1469b9fd7044cbe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"wrap-up\">\n",
    "    <div class=\"wrap-up-title\">Wrap up - Stochastic Gradient Descent</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "        <ul>\n",
    "          <li>Vanilla or batch GD computes the gradient of the loss function w.r.t. to the parameters (weights) for the entire training dataset. Therefore, to perform just one update, the gradient of the whole dataset needs to be computed, which makes batch GD very slow.</li>\n",
    "          <li>SGD, in contrast, computes the gradient of the loss function and performs updates for each data point.</li>\n",
    "          <li>SGD performs faster parameter updates, but suffers from high variance, especially with a constant learning rate.</li>\n",
    "          <li>Mini-batch GD computes the gradient of the loss function for every batch of the dataset, thus combines the benefits of both, batch GD and SGD. It is faster than batch GD but prone to less variation in parameters up-dates than SGD.</li>\n",
    "          </ul>\n",
    "          <br>\n",
    "        Note, that in deep learning literature mini-batch GD sometimes is also called SGD.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eec90a819619a7383829771e036336cd",
     "grade": false,
     "grade_id": "cell-5ed712c900639704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Variants of Gradient-Based Optimization Algorithms\n",
    "\n",
    "Besides plain GD or mini-batch GD, many deep learning methods use somewhat more advanced variants of gradient-based algorithms or optimizers ([list](https://keras.io/api/optimizers/) of optimizers available in deep learning Python library Keras). Some of the most known are SGD with momentum, RMSprop, and Adam. \n",
    "\n",
    "Much like GD and SGD, these algorithms use gradients of the loss function $f(\\mathbf{w})$ to find weights $\\mathbf{w}$ such that the predictor $h^{(\\mathbf{w})}$ achieves (nearly) minimum loss. These variants differ in how they use (or \"interpret\") the gradient information to find the fastest route towards the minimum. In some cases, these variants can find good weight vectors significantly faster (using fewer iterations) compared to mini-batch GD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "580c9694addc1cbfc2551ccb7cebaecf",
     "grade": false,
     "grade_id": "cell-5d0495b1535d7fee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Gradient Descent with momentum\n",
    "\n",
    "One simple improvement of gradient descent is adding a \"momentum\" term to the weight update equation. GD manages poorly when the loss surface is much steeper in one dimension than in the other, e.g. in ravines of the loss function. You can see on the animation below loss \"landscape\": weights values are shown on x and y axes and loss values are shown on the z-axis. GD (black trace) bounces back and force in the narrow symmetric ravine, unable to converge to the minimum of the loss function.\n",
    "\n",
    "The addition of the \"momentum\" helps to damp oscillations and accelerate in the relevant direction. The common analogy is the ball moving in a bowl and gaining speed and momentum in the downhill direction, rather than moving to the left or right on the sides of the bowl.\\\n",
    "GD with momentum is implemented by storing the information about past gradients in a vector ${v}_{t}=\\beta*{v}_{t-1} + \\alpha*\\nabla f$.\\\n",
    "The weight is updated as follows:\n",
    "${w}_{t}={w}_{t-1}-{v}_{t}$\n",
    "\n",
    "Momentum will increase for dimensions where gradients will point in the same direction and will decrease for dimensions where gradients change direction (oscillates). You can see on the animation below, how GD with momentum (red trace) decreases \"zigzagging\" amplitude and moves faster in the direction of the minimum of the loss function. Note that it also overshoots the minimum, but \"corrects\" itself and moves back converging to the minimum. Setting $\\beta$ to a lower value would decrease momentum and would prevent overshooting. \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/GDMomentum2.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7768723172d867f7e1a57a3b837f608",
     "grade": false,
     "grade_id": "cell-62eb6d4e5d148347",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below you can see how GD with momentum (red trace) gained momentum after the first weights update, escaped the ravine, and \"hopped\" over \"hill\" (the area where loss values are higher). In this case, it may be not an optimal path to the minimum, but in the more complex landscapes momentum can help to escape the local minimum given big enough momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db1df4aa4755c0dde374119c40ff82f8",
     "grade": false,
     "grade_id": "cell-e7d5abd78e65e01b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"../../../coursedata/SGD/GDMomentumRosenbrock2.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed6ddc39cadd0f3f3dae50677c677801",
     "grade": false,
     "grade_id": "cell-2cc4fbad06a334ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Other variants of GD improve different aspects of the algorithm. Nesterov accelerated gradient is basically a modification of the GD with momentum. Adagrad and its extension Adadelta uses different learning rates for parameters, which is useful when dealing with sparse data. RMSprop and Adam are currently a popular choice of GD-based variants with an adaptive learning rate ([paper about GD variants](https://arxiv.org/pdf/1609.04747.pdf)). \n",
    "\n",
    "The animation below compares the \"routes\" taken by several optimizers to find a minimum of the [six-hump camel](https://www.sfu.ca/~ssurjano/camel6.html) function. As you can see different GD variants may take different paths, which can be controlled by hyperparameters tuning (such as learning rate and coefficients like $\\beta$ in GD with momentum).\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/camel3D.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60729db46582c1cfc7f127b2eff44160",
     "grade": false,
     "grade_id": "cell-ab7d99814fb54024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St8'></a>\n",
    "<div class=\" alert alert-success\">\n",
    "     <h3><b>STUDENT TASK 2.8. (OPTIONAL, not graded). </b> Learning Rate Schedule. </h3>\n",
    "    \n",
    "Your task is to modify <code> minibatchGD()</code> function and Learning rate schedule to mitigate the accumulation of the gradient noise. For example, you can update learning rate on each iteration <code>lrate = lrate * decay_rate**(current_iteration / decay_steps)</code> as described in [Keras ExponentialDecay docs](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/).\n",
    "    \n",
    "To test your modified function `minibatchGD_lrate()` use the same code as in \"STUDENT TASK 2.7. SGD - BATCH SIZE.\" and plot loss curves for different batch sizes.\n",
    "    \n",
    "You can also introduce stopping criteria as in \"STUDENT TASK 2.5. STOPPING CRITERIA.\"\n",
    "You will need to compare loss values returned on the last two epochs and stop GD when the loss will stop decreasing significantly (e.g. the loss decrease is less than 1e-6). In addition, you can introduce a maximum number of epochs GD can perform, in order to prevent GD from running for a long time. The function should return last weight(s), loss, and the number of epochs performed.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f659c1f9b1e4590b187275bf82354fc",
     "grade": false,
     "grade_id": "cell-a75ea32fc9fc7bb7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def minibatchGD_lrate(X, y, batch_size, epochs, lrate):  \n",
    "    \n",
    "    # initialize the weight randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand()  \n",
    "    decay = 0.96\n",
    "    decay_steps = 100\n",
    "    # list to store the loss values \n",
    "    losses = []\n",
    "    # list to store the weight values \n",
    "    weights = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "        # Use another for-loop to iterate batch() generator and access batches one-by-one\n",
    "            # Update learning rate with use of exponential decay\n",
    "            # Feed  current batch to `gradient_step_onefeature()` and get weight and loss values\n",
    "            # Save current weight and loss values in corresponding lists\n",
    "      \n",
    "        # one epoch is finished when the algorithm goes through ALL batches\n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee444ca83a01282f8253648fbde6753e",
     "grade": false,
     "grade_id": "cell-907425396a084160",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Try out different decay rates in the interval [0,1] and see how it affects loss curves for different batch size options.\n",
    "\n",
    "When training models in Keras you can pass learning rate scheduler as an optimizer parameter (read more in [Keras docs](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/)):\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36bcbc759f16bf7dd285aec3a773a127",
     "grade": false,
     "grade_id": "cell-4db4e564f64fd90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Conclusion \n",
    "\n",
    "We have discussed the basic idea of using gradients of loss functions to iteratively improve the parameter values (weights) in a predictor map. Gradient-based methods such as SGD and its variants turn out to be the perfect tool for training deep neural networks in several aspects. First, somewhat surprisingly, SGD quickly finds weights for an ANN such that it performs well on new data points which are different from the training data. Moreover, mini-batch GD requires only to have enough working memory (\"RAM\") to store the current batch (subset) of training data points instead of the entire dataset (which might be billions of high-resolution images). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0669978bbfdd8d2a39062caf2295049",
     "grade": false,
     "grade_id": "cell-d67f67c973361771",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quiz\n",
    "[0.25 points each]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57244c86a5f9df2670013eddc500fecd",
     "grade": false,
     "grade_id": "cell-826b0d68236d3ec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.1.</b></h3>\n",
    "\n",
    "What is the role of a loss function within machine learning methods?\n",
    "         \n",
    "Select one:\n",
    "\n",
    "1. To reduce the memory requirements of a machine learning method.\n",
    "\n",
    "2. To transform the data into a computer-friendly format.\n",
    "\n",
    "3. To speed up the learning process of an ML method.\n",
    "\n",
    "4. To measure the quality of a particular predictor map.\n",
    "         \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cabe9dcc5d4d239fec8fd1f17555127e",
     "grade": false,
     "grade_id": "cell-1d23029c8fc0c549",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_21  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c07babf672b6f87ef019d110057abf77",
     "grade": true,
     "grade_id": "cell-1c14f10dcabd0e6d",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_21 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "448960a938a70b58e493155a4a4fa6d8",
     "grade": false,
     "grade_id": "cell-173eaa264e18d0d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.2.</b></h3>\n",
    "         \n",
    "Consider the average loss, or training loss, incurred by a predictor map on a set of labeled data points (the training set). The predictor map involves some adjustable weights. Which of the following statements is correct?\n",
    "Select one:\n",
    "\n",
    "1. The training loss (average loss incurred on training data points) depends only on the labels.\n",
    "\n",
    "2. The training loss does not depend on the weights of the predictor map.\n",
    "\n",
    "3. The training loss does not depend on the features of the data points in the training set.\n",
    "\n",
    "4. The training loss depends on the features, labels of training data points as well as on weights of the predictor map.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17d4d36d582e39d6bca49a932d6967c9",
     "grade": false,
     "grade_id": "cell-3f3a40ae966607fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_22  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50676bc53b010f1eb1a97ae466bac54f",
     "grade": true,
     "grade_id": "cell-4ba09434dbdc1355",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_22 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5a8cf9b6c4a1aa6861dd59ea04928b8",
     "grade": false,
     "grade_id": "cell-78633828d2da2eaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.3.</b></h3>\n",
    "\n",
    "Which one of the following completions of the sentence below is correct? Gradient descent is a ...\n",
    "\n",
    "1. method to visualize data points.\n",
    "\n",
    "2. iterative algorithm for finding a good choice for the weights (parameters) of a predictor map.\n",
    "\n",
    "3. method to avoid overfitting.\n",
    "\n",
    "4. method to divide the data into batches.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31ee0484af6761f26bdaf645acc2d108",
     "grade": false,
     "grade_id": "cell-1419b5a420cfa16a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_23  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "205cccf8a0b95e591cab55a51faf64d7",
     "grade": true,
     "grade_id": "cell-26f1ff9837b88b9f",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_23 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ee404b07f699f472956e56b27ea04dc",
     "grade": false,
     "grade_id": "cell-386c530d9dbd20df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.4.</b></h3>\n",
    "\n",
    "Consider a single GD step (update) $\\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}−\\alpha \\nabla f(\\mathbf{w}^{(k)})$ using the training loss $f(\\mathbf{w})$ incurred by an ANN with parameters $\\mathbf{w}$ on $m$ labeled training data points $(\\mathbf{x}^{(1)},{y}^{(1)}),…,(\\mathbf{x}^{(m)},{y}^{(m)})$. The function $f(\\mathbf{w})$ is known to be differentiable and convex. The GD update includes an adjustable (tunable) parameter $\\alpha>0$ which is referred to as step size or learning rate. Which of the following statements is correct?\n",
    "\n",
    "Select one:\n",
    "\n",
    "1. The GD iterates $\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\mathbf{w}^{(2)},…$ converge always, no matter what our choice is for $\\alpha$.\n",
    "\n",
    "2. The GD iterates can only converge if the feature vectors $\\mathbf{x}^{(1)},…,\\mathbf{x}^{(m)}$ have a norm smaller than one. \n",
    "\n",
    "3. If we choose very **small** learning rate $\\alpha$, GD iterates will converge anyway (assuming infinite time and computational resources).\n",
    "\n",
    "4. If we choose very **large** learning rate $\\alpha$, GD iterates will converge anyway (assuming infinite time and computational resources).   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28aa60d2a5e98f1ba9fb8219715b205c",
     "grade": false,
     "grade_id": "cell-86eff0cd2fb8da77",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_24  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "328e7c3a9f4aad646226b86d23c69ca2",
     "grade": true,
     "grade_id": "cell-3ce60ca04b6476c3",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_24 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f86f7395e500be23587c62788fc9b209",
     "grade": false,
     "grade_id": "cell-07b1778475a2511d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.5.</b></h3>\n",
    "\n",
    "Pick the correct completion for the following sentence. \n",
    "\n",
    "In contrast to plain gradient descent (GD), stochastic gradient descent (SGD) ...\n",
    "\n",
    "\n",
    "1. is guaranteed to strictly decrease the training error after each iteration.\n",
    "\n",
    "2. always learns ANN parameters resulting in smaller training errors than the parameters learnt by GD.\n",
    "\n",
    "3. can only be used for ANNs with more than 100 layers.  \n",
    "\n",
    "4. typically requires less computation (time).   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdfa0df9c809153c644599178238cfd8",
     "grade": false,
     "grade_id": "cell-874a654a73538007",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_25  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1099f3c45607a5cec4f5b7519ecdc74e",
     "grade": true,
     "grade_id": "cell-3192673c0457a226",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_25 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "464ff0428bf454d73e6fafbf729fdbab",
     "grade": false,
     "grade_id": "cell-2f38143a4eee875b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.6.</b></h3>\n",
    "\n",
    "Which one of the following statements is correct?\n",
    "\n",
    "1. The batch size in SGD has no influence on the prediction accuracy obtained with the learnt ANN parameters. \n",
    "\n",
    "2. A single iteration of gradient descent updates the parameter vector into the opposite direction of the gradient of the loss function.\n",
    "\n",
    "3. A single iteration of gradient descent updates the parameter vector into the direction of the gradient of the loss function.\n",
    "\n",
    "4. In the context of SGD, one epoch refers to one single update of the ANN parameters based on a noisy gradient estimate computed over a batch.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f06ec23ed37b63691b36a7f3d435918",
     "grade": false,
     "grade_id": "cell-f197d4a928f590ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_26  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d2add5467f7b5d622335e552206be3",
     "grade": true,
     "grade_id": "cell-25e60f29e9109354",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_26 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b3c5f51ad664ffcd460b7ec7277740e",
     "grade": false,
     "grade_id": "cell-10d7b11165411e7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.7.</b></h3>\n",
    "    \n",
    "Consider a dataset consisting of 4800 data points $({x}^{(1)},{y}^{(1)}),…,({x}^{(4800)},{y}^{(4800)})$.\n",
    "If we run mini-batch GD with a batch size of 64 and for 100 epochs, what is the total number of iterations used in GD? \n",
    "         \n",
    "Select one option:\n",
    "\n",
    "1. 30\n",
    "\n",
    "2. 160000\n",
    "\n",
    "3. 15\n",
    "\n",
    "4. 7500\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e523b7143dde230214b842009a03155a",
     "grade": false,
     "grade_id": "cell-b24bba98b125d531",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_27  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "979bc1ddbdf94d66db09a7f03fd492fb",
     "grade": true,
     "grade_id": "cell-d8317ef591c65e02",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_27 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 2,
           "op": "addrange",
           "valuelist": "8"
          },
          {
           "key": 2,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.136017px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
